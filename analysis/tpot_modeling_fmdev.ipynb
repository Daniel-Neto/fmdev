{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "from dask.distributed import Client\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_default():\n",
    "    df_temp = pd.read_csv(\"data/base.csv\", sep=';')\n",
    "\n",
    "    df_temp = df_temp[df_temp['Curso'] == 'Biologia']\n",
    "\n",
    "    df = df_temp[['VAR01', 'VAR02', 'VAR03', 'VAR04', 'VAR05',\n",
    "                  'VAR06', 'VAR07', 'VAR08', 'VAR09', 'VAR10',\n",
    "                  'VAR12', 'VAR14', 'VAR15', 'VAR16', 'VAR17',\n",
    "                  'VAR18', 'VAR19', 'VAR20', 'VAR21', 'VAR22',\n",
    "                  'VAR23', 'VAR24', 'VAR25', 'VAR28', 'VAR30',\n",
    "                  'VAR31', 'VAR31b', 'VAR31c', 'VAR32a', 'VAR32b',\n",
    "                  'VAR32c', 'VAR32d', 'VAR33', 'VAR34', 'VAR35', 'DESEMPENHO_BINARIO']]\n",
    "\n",
    "    df = df.rename(columns={\"DESEMPENHO_BINARIO\": \"Y\"})\n",
    "\n",
    "    df[\"VAR17\"] = df[\"VAR17\"].astype(str).replace(',','.')\n",
    "    df[\"VAR24\"] = df[\"VAR24\"].astype(str).replace(',','.')\n",
    "    df[\"VAR25\"] = df[\"VAR25\"].astype(str).replace(',','.')\n",
    "    df[\"VAR30\"] = df[\"VAR30\"].astype(str).replace(',','.')\n",
    "\n",
    "    df[\"VAR17\"] = df[\"VAR17\"].astype(float)\n",
    "    df[\"VAR24\"] = df[\"VAR24\"].astype(float)\n",
    "    df[\"VAR25\"] = df[\"VAR25\"].astype(float)\n",
    "    df[\"VAR30\"] = df[\"VAR30\"].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_fmdev():\n",
    "    df = pd.read_csv(\"../backend/data/raw/94ca2547-88c4-4c10-9446-c71a09dc579e.csv\")\n",
    "    \n",
    "#     df_categoric = df.copy()\n",
    "#     df_categoric = df_categoric.select_dtypes(include=['object'])\n",
    "#     df = df.drop(df_categoric.columns, axis=1)\n",
    "#     del df['id_do_aluno']\n",
    "\n",
    "        \n",
    "    \n",
    "#     df = df[['var01', 'var02', 'var03', 'var04', 'var05',\n",
    "#                   'var06', 'var07', 'var08', 'var09', 'var10',\n",
    "#                   'var12', 'var14', 'var15', 'var16', 'var17',\n",
    "#                   'var18', 'var19', 'var20', 'var21', 'var22',\n",
    "#                   'var23', 'var24', 'var25', 'var28', 'var30',\n",
    "#                   'var31', 'var31b', 'var31c', 'var32a', 'var32b',\n",
    "#                   'var32c', 'var32d', 'var33', 'var34', 'var35', 'desempenho_binario']]\n",
    "\n",
    "#     df = df.rename(columns={\"desempenho_binario\": \"y\"})\n",
    "\n",
    "#     df[\"var17\"] = df[\"var17\"].astype(str).replace(',','.')\n",
    "#     df[\"var24\"] = df[\"var24\"].astype(str).replace(',','.')\n",
    "#     df[\"var25\"] = df[\"var25\"].astype(str).replace(',','.')\n",
    "#     df[\"var30\"] = df[\"var30\"].astype(str).replace(',','.')\n",
    "\n",
    "#     df[\"var17\"] = df[\"var17\"].astype(float)\n",
    "#     df[\"var24\"] = df[\"var24\"].astype(float)\n",
    "#     df[\"var25\"] = df[\"var25\"].astype(float)\n",
    "#     df[\"var30\"] = df[\"var30\"].astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(df, target):\n",
    "    msk = np.random.rand(len(df)) < 0.7\n",
    "\n",
    "    train = df[msk]\n",
    "    test = df[~msk]\n",
    "\n",
    "    Y_test = test[target]\n",
    "    X_test = test.drop(labels = [target],axis = 1)\n",
    "\n",
    "    Y_train = train[target]\n",
    "    X_train = train.drop(labels = [target],axis = 1)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df_from_fmdev()\n",
    "X_train, Y_train, X_test, Y_test = get_split(df, 'desempenho_binario')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=20.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.8834953012311502\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=9, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Saving periodic pipeline from pareto front to data/tpot-output/pipeline_gen_1_idx_0_2020.04.10_21-13-17.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.891593210461135\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=14, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.35000000000000003)\n",
      "\n",
      "Saving periodic pipeline from pareto front to data/tpot-output/pipeline_gen_2_idx_0_2020.04.10_21-14-02.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.8926684636118598\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=14, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "-2\t0.8932075471698113\tGradientBoostingClassifier(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=10, GradientBoostingClassifier__max_features=0.7500000000000001, GradientBoostingClassifier__min_samples_leaf=12, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2)\n",
      "\n",
      "Saving periodic pipeline from pareto front to data/tpot-output/pipeline_gen_3_idx_0_2020.04.10_21-14-33.py\n",
      "Saving periodic pipeline from pareto front to data/tpot-output/pipeline_gen_3_idx_1_2020.04.10_21-14-33.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 feature_names mismatch: ['var01', 'var02', 'var03', 'var04', 'var05', 'var06', 'var07', 'var08', 'var09', 'var10', 'var12', 'var13', 'var14', 'var15', 'var16', 'var17', 'var18', 'var19', 'var20', 'var21', 'var22', 'var23', 'var24', 'var25', 'var28', 'var29', 'var30', 'var31', 'var31b', 'var31c', 'var32a', 'var32b', 'var32c', 'var32d', 'var33', 'var34', 'var35'] ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36']\n",
      "expected var07, var28, var18, var32b, var31c, var33, var23, var32d, var20, var19, var32a, var31b, var03, var14, var17, var10, var24, var31, var02, var09, var25, var30, var22, var12, var35, var01, var04, var05, var32c, var16, var13, var06, var21, var15, var29, var34, var08 in input data\n",
      "training data did not have the following fields: f10, f17, f29, f6, f13, f8, f9, f32, f33, f1, f12, f35, f27, f15, f19, f34, f31, f21, f0, f22, f11, f23, f36, f14, f20, f4, f5, f7, f18, f26, f28, f2, f3, f24, f16, f25, f30.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.8932119181175786\tGradientBoostingClassifier(CombineDFs(input_matrix, input_matrix), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=14, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "\n",
      "Saving periodic pipeline from pareto front to data/tpot-output/pipeline_gen_4_idx_0_2020.04.10_21-15-16.py\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.8958985940118016\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=12, GradientBoostingClassifier__min_samples_split=14, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.35000000000000003)\n",
      "\n",
      "Saving periodic pipeline from pareto front to data/tpot-output/pipeline_gen_5_idx_0_2020.04.10_21-15-52.py\n",
      "\r"
     ]
    }
   ],
   "source": [
    "# connect to the cluster\n",
    "client = Client(processes=False) \n",
    "\n",
    "# create the estimator normally\n",
    "estimator = TPOTClassifier(generations=5, max_time_mins=5, population_size=20, cv=5,\n",
    "                                    random_state=42, verbosity=3, \n",
    "                                    use_dask=True,\n",
    "                                    periodic_checkpoint_folder=\"data/tpot-output/\")\n",
    "\n",
    "# perform the fit in this context manager\n",
    "with joblib.parallel_backend(\"dask\"):\n",
    "    estimator.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8987012987012987"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-82c63747267f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpareto_front_fitted_pipelines_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "for item in estimator.pareto_front_fitted_pipelines_:\n",
    "    print(item.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-a17986d889e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpareto_front_fitted_pipelines_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "estimator.pareto_front_fitted_pipelines_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
